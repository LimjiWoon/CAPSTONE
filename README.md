# 와셔 탐지 및 분류

이 프로젝트는 이미지에서 와셔를 탐지하고 여러 사전 학습된 모델을 사용하여 분류하도록 설계되었습니다. 초기 탐지를 위해 YOLOv5 객체 탐지 프레임워크를 활용한 후 ResNet18, ResNet34, VGG16, MobileNet의 소프트 보팅 앙상블을 사용하여 분류합니다.

## 설정

1. YOLOv5 리포지토리를 클론합니다:
    ```bash
    git clone https://github.com/ultralytics/yolov5
    ```

2. 이 프로젝트에서 제공된 수정된 `detect.py` 파일로 YOLOv5 리포지토리의 `detect.py`를 교체합니다.

3. 필요한 모든 종속성을 설치합니다:
    ```bash
    pip install -r requirements.txt
    ```

## 파일

### detect.py
이 파일은 YOLOv5의 `detect.py`의 수정된 버전입니다. 객체를 탐지하는 것 외에도 탐지된 영역을 잘라서 이러한 잘린 이미지를 저장합니다.

### main.py
이 파일은 메인 프로그램 파일입니다. 다음 작업을 수행합니다:
1. 수정된 `detect.py`를 사용하여 지정된 폴더에 있는 이미지에서 와셔를 탐지합니다.
2. 탐지 결과와 잘린 이미지를 `result` 폴더에 저장합니다.
3. 잘린 이미지를 사전 학습된 모델(ResNet18, ResNet34, VGG16, MobileNet)의 앙상블을 사용하여 분류하고 소프트 보팅을 통해 최종 분류 결과를 도출합니다.
4. 결과를 실시간 UI에 업데이트합니다.

## 사용법

1. YOLOv5 리포지토리가 클론되고 위의 지침에 따라 `detect.py` 파일이 교체되었는지 확인합니다.
2. 메인 프로그램을 실행합니다:
    ```bash
    python main.py
    ```
3. 프로그램은 지정된 폴더에서 이미지를 처리하고, 와셔를 탐지하며, 결과와 잘린 이미지를 저장하고, 잘린 이미지를 모델 앙상블을 사용하여 분류합니다.

## 앙상블 모델 결과

ResNet18, ResNet34, ResNet50, VGG16, MobileNet 모델의 다양한 조합을 소프트 보팅 앙상블로 사용한 결과는 아래에 요약되어 있습니다. 가장 성능이 좋은 조합은 ResNet34와 MobileNet입니다.

### 두 가지 모델 조합

| 모델 조합       | True 0 | True 1 | 정확도    |
|----------------|--------|--------|-----------|
| 18, 34         | 120    | 184    | 76.44%    |
| 18, 50         | 163    | 158    | 80.20%    |
| 18, VGG        | 161    | 153    | 78.95%    |
| 18, Mobile     | 195    | 172    | 91.97%    |
| 34, 50         | 113    | 184    | 74.44%    |
| 34, VGG        | 93     | 186    | 69.92%    |
| 34, Mobile     | 187    | 188    | 93.98%    |
| 50, VGG        | 160    | 157    | 79.45%    |
| 50, Mobile     | 196    | 174    | 92.73%    |
| VGG, Mobile    | 193    | 170    | 91.23%    |

### 세 가지 모델 조합

| 모델 조합       | True 0 | True 1 | 정확도    |
|-----------------|--------|--------|-----------|
| 18, 34, 50      | 138    | 177    | 78.94%    |
| 18, 34, Mobile  | 165    | 182    | 86.96%    |
| 18, 50, Mobile  | 181    | 168    | 87.46%    |
| 34, 50, Mobile  | 161    | 182    | 85.71%    |
| 18, VGG, Mobile | 193    | 58     | 62.91%    |
| 34, VGG, Mobile | 175    | 67     | 60.65%    |
| 50, VGG, Mobile | 189    | 55     | 61.40%    |
| 18, 34, VGG     | 122    | 184    | 76.44%    |
| 18, 50, VGG     | 174    | 118    | 73.44%    |
| 34, 50, VGG     | 151    | 162    | 78.90%    |

### 네 가지 모델 조합

| 모델 조합           | True 0 | True 1 | 정확도    |
|--------------------|--------|--------|-----------|
| 18, 34, 50, Mobile | 167    | 174    | 85.71%    |
| 18, 34, 50, VGG    | 138    | 177    | 78.94%    |
| 18, 34, VGG, Mobile| 166    | 181    | 86.71%    |
| 18, 50, VGG, Mobile| 181    | 168    | 87.46%    |
| 34, 50, VGG, Mobile| 160    | 182    | 85.21%    |

### 모든 5가지 모델 조합

| 모델 조합                     | True 0 | True 1 | 정확도    |
|------------------------------|--------|--------|-----------|
| 18, 34, 50, VGG, Mobile      | 167    | 174    | 85.71%    |

## 결론

결과에서 알 수 있듯이 ResNet34와 MobileNet의 조합이 93.98%로 가장 높은 정확도를 보였습니다. 따라서 ResNet34와 MobileNet을 함께 사용하는 것이 최고의 성능을 내기 위해 권장됩니다.
